# Evaluation Module Documentation

## Overview

The `evaluation` module provides tools for assessing the performance of the Advanced RAG system. It includes functionality for comparing RAG responses to ground truth, detecting novel insights, and evaluating all queries in the dataset. The module also supports updating ground truth data with expert-validated insights.

## Key Features

- **Query Evaluation**: Compares RAG responses to ground truth using text-based similarity metrics.
- **Novel Insight Detection**: Identifies novel approaches or insights in RAG responses.
- **Batch Evaluation**: Evaluates all queries in the dataset and generates detailed results.
- **Ground Truth Management**: Allows updating ground truth data with expert-validated insights.

## File Structure

evaluation/
├── __init__.py            # Module initializer
├── eval_service.py        # Core evaluation logic and API routes
└── evaluation_readme.txt  # Documentation for the evaluation module

## Functions in `eval_service.py`

### 1. `evaluate_query(query_id=None, query_text=None)`
Evaluates a single query by comparing the RAG response to the ground truth.

- **Input**:
  - `query_id`: ID of the query to evaluate
  - `query_text`: Text of the query to evaluate (optional)
- **Output**: A dictionary containing:
  - `query_id`, `query`, `ground_truth`, `rag_response`
  - `similarity_scores`: Text-based similarity metrics
  - `novel_insight_detected`: Boolean indicating if novel insights were found
  - `novel_insight`: Details of the detected novel insights (if any)

### 2. `evaluate_all_queries()`
Evaluates all queries in the ground truth dataset.

- **Input**: None
- **Output**: A list of evaluation results for all queries.

### 3. `simple_compare_response(rag_response, ground_truth)`
Performs a simple text-based comparison between the RAG response and the ground truth.

- **Input**:
  - `rag_response`: The response generated by the RAG system
  - `ground_truth`: The expected response or data
- **Output**: A dictionary of similarity scores for each key in the ground truth.

### 4. `simple_detect_novel_insight(rag_response, ground_truth)`
Detects novel insights in the RAG response by identifying terms or sentences not present in the ground truth.

- **Input**:
  - `rag_response`: The response generated by the RAG system
  - `ground_truth`: The expected response or data
- **Output**: A dictionary containing:
  - `novel_sentences`: Sentences containing novel insights
  - `explanation`: Explanation of why these sentences are considered novel

### 5. `update_novel_insights(query_id, novel_insight, expert_rating)`
Updates the ground truth dataset with expert-validated novel insights.

- **Input**:
  - `query_id`: ID of the query to update
  - `novel_insight`: The novel insight to add
  - `expert_rating`: Expert rating for the novel insight
- **Output**: A dictionary indicating success or failure.

## API Endpoints

### `GET /health`
Checks the health of the evaluation service.

**Response:**
```json
{
  "status": "healthy"
}
```

### `POST /evaluate`
Evaluates a single query.

**Request:**
```json
{
  "query_id": "example_query_id",
  "query_text": "example query text"
}
```

**Response:**
```json
{
  "query_id": "example_query_id",
  "similarity_scores": {...},
  "novel_insight_detected": true,
  "novel_insight": {...}
}
```

### `GET /evaluate-all`
Evaluates all queries in the ground truth dataset.

**Response:**
```json
{
  "results": [...]
}
```

### `POST /update-novel-insight`
Updates the ground truth with a novel insight.

**Request:**
```json
{
  "query_id": "example_query_id",
  "novel_insight": "example novel insight",
  "expert_rating": 5
}
```

**Response:**
```json
{
  "status": "success",
  "message": "Novel insight saved"
}
```

## Environment Variables

- `LLM_SERVICE_URL`: URL of the LLM service (default: `http://llm-service:8000`)
- `GROUND_TRUTH_PATH`: Path to the ground truth JSON file

These variables should be set in a `.env` file in the root directory:
```plaintext
LLM_SERVICE_URL=http://llm-service:8000
GROUND_TRUTH_PATH=/path/to/ground_truth.json
```

### Example Usage

Evaluating a Single Query

```python
from evaluation.eval_service import RAGEvaluator

evaluator = RAGEvaluator()
result = evaluator.evaluate_query(query_id="example_query_id")
print(result)
```

Detecting Novel Insights

```python
from evaluation.eval_service import RAGEvaluator

evaluator = RAGEvaluator()
novel_insight = evaluator.simple_detect_novel_insight(
    rag_response="This is a novel approach to training.",
    ground_truth={"response": "Standard training methods"}
)
print(novel_insight)
```

```python
from evaluation.eval_service import RAGEvaluator

evaluator = RAGEvaluator()
novel_insight = evaluator.simple_detect_novel_insight(
    rag_response="This is a novel approach to training.",
    ground_truth={"response": "Standard training methods"}
)
print(novel_insight)
```

## Notes

- Ensure the `GROUND_TRUTH_PATH` environment variable points to the correct ground truth JSON file.
- The novel insight detection is currently text-based and does not use embeddings.
- Batch evaluation results can be exported for further analysis.